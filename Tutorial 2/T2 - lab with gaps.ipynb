{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Tutorial 2\n",
    "\n",
    "In the last tutorial we implemented single-layered neural networks using PyTorch, and we also implemented our own training algorithm.\n",
    "\n",
    "In this tutorial we would like to experiment by implementing neural networks that have at least one hidden layer, and we would like to start utilising PyTorch's implementation of the backpropogation algorithm to train it rather than using our own .\n",
    "\n",
    "In particular, we will implement a multi-layer neural network that solves a non-trivial differential equation similar to the kind of equations that arise in many applications such as robotics and autonomous vehicles.\n",
    "\n",
    "The following is the problem we want to solve using the neural network that we will implement:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pole.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solve this problem using a multilayer neural network we will utilise the following dataset, which consists of 200 random samples of (angle, angular velocity) pairs, and for each one of these pairs we will also have the value of F that solves equation 1 and 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "pole_data = [\n",
    "[0.150833,1.027983,1.779294],\n",
    "[-1.003966,0.134535,-4.083502],\n",
    "[-1.004733,1.277974,-2.942121],\n",
    "[-0.901717,0.066560,-3.855405],\n",
    "[0.073415,1.333916,1.700663],\n",
    "[-0.686864,-1.141066,-4.311643],\n",
    "[-0.982299,-0.835420,-4.994299],\n",
    "[-1.435255,-1.071030,-6.025172],\n",
    "[-0.415685,0.499814,-1.519269],\n",
    "[0.534137,0.549333,3.094825],\n",
    "[1.451074,-0.662416,4.301793],\n",
    "[0.011577,-0.418178,-0.360295],\n",
    "[0.574931,-0.289563,2.429322],\n",
    "[0.935129,1.518234,5.541612],\n",
    "[-0.704409,-0.487015,-3.724932],\n",
    "[-0.863463,1.017197,-2.783291],\n",
    "[-1.029852,-0.296658,-4.582772],\n",
    "[-0.419088,-0.463813,-2.498453],\n",
    "[-0.589456,0.249967,-2.529578],\n",
    "[1.173519,1.054732,5.665321],\n",
    "[0.013063,-1.084117,-1.018805],\n",
    "[1.484438,0.601584,5.582951],\n",
    "[-0.348765,-1.521637,-3.230324],\n",
    "[1.101326,0.806323,5.265364],\n",
    "[0.548997,0.064643,2.673804],\n",
    "[-0.215117,0.429251,-0.638057],\n",
    "[0.972999,1.367232,5.500120],\n",
    "[-1.559747,-0.022842,-5.022537],\n",
    "[-0.493151,0.946202,-1.420816],\n",
    "[-0.075429,0.373213,-0.003573],\n",
    "[-1.111633,0.631928,-3.850189],\n",
    "[-0.180411,-0.570713,-1.467880],\n",
    "[-1.235550,0.971273,-3.750372],\n",
    "[0.536294,-0.254233,2.300536],\n",
    "[-0.349532,0.138993,-1.573297],\n",
    "[-0.770322,1.234303,-2.247529],\n",
    "[0.625648,-0.856656,2.071459],\n",
    "[0.265067,-1.293889,0.015981],\n",
    "[-0.807521,-0.204427,-3.817305],\n",
    "[1.083206,1.312249,5.729571],\n",
    "[1.430988,-0.702731,4.248483],\n",
    "[0.170727,0.833215,1.682711],\n",
    "[-0.906319,0.181753,-3.754443],\n",
    "[-0.760447,0.171303,-3.274924],\n",
    "[-0.442865,0.734897,-1.407753],\n",
    "[-1.026305,0.016275,-4.260680],\n",
    "[-0.203995,0.364057,-0.648861],\n",
    "[1.016382,0.131275,4.382320],\n",
    "[-0.235442,-0.018144,-1.184509],\n",
    "[1.447814,0.985798,5.948035],\n",
    "[-1.449971,-0.893280,-5.856828],\n",
    "[0.649281,0.746497,3.769568],\n",
    "[-0.179164,-0.656424,-1.547460],\n",
    "[1.023381,0.584087,4.853450],\n",
    "[0.709922,0.535767,3.794638],\n",
    "[0.325563,0.570138,2.169351],\n",
    "[1.403856,-1.074530,3.855959],\n",
    "[-0.167468,-1.073283,-1.906713],\n",
    "[0.677996,0.642858,3.779024],\n",
    "[0.668840,-1.335642,1.764738],\n",
    "[-0.193066,1.213307,0.253963],\n",
    "[0.251405,1.173759,2.417584],\n",
    "[0.006591,-0.303033,-0.270077],\n",
    "[-0.265786,1.341922,0.028582],\n",
    "[1.249595,-0.388792,4.355492],\n",
    "[0.756900,1.370444,4.803799],\n",
    "[0.288700,-0.164639,1.258892],\n",
    "[0.546121,-1.461284,1.135598],\n",
    "[0.749709,-0.001318,3.405811],\n",
    "[0.693623,-0.111142,3.085494],\n",
    "[-1.036372,-0.551634,-4.854443],\n",
    "[-1.111824,-1.203336,-5.685878],\n",
    "[-0.055391,0.291480,0.014666],\n",
    "[-0.705799,-0.948216,-4.191426],\n",
    "[-0.636482,1.533813,-1.438038],\n",
    "[-0.713085,0.741224,-2.529628],\n",
    "[1.176300,1.109140,5.725091],\n",
    "[0.344163,-0.387929,1.299115],\n",
    "[-0.764713,-1.492443,-4.954095],\n",
    "[-0.616828,-1.085891,-3.978144],\n",
    "[-0.310415,-1.430749,-2.958020],\n",
    "[-1.286267,1.549057,-3.249912],\n",
    "[-0.024568,0.830627,0.707801],\n",
    "[-1.483000,-0.845679,-5.826421],\n",
    "[-0.741512,0.781395,-2.595623],\n",
    "[0.613952,-0.207111,2.673399],\n",
    "[-1.341059,1.072900,-3.795731],\n",
    "[0.160373,0.174323,0.972754],\n",
    "[-0.206440,1.025346,0.000461],\n",
    "[0.796879,0.727850,4.303741],\n",
    "[0.988339,-1.486979,2.688589],\n",
    "[-0.101698,0.593818,0.086204],\n",
    "[1.192934,-1.328355,3.318921],\n",
    "[-1.364883,-1.142600,-6.036974],\n",
    "[-1.249979,-0.410939,-5.155828],\n",
    "[-0.657718,0.010378,-3.046185],\n",
    "[-0.270867,-0.373213,-1.711050],\n",
    "[-0.011385,1.275337,1.218413],\n",
    "[-1.113406,0.076387,-4.409653],\n",
    "[-1.141162,-0.284146,-4.829736],\n",
    "[-0.713037,1.043610,-2.227061],\n",
    "[1.079563,-0.483324,3.925435],\n",
    "[0.545690,-0.330884,2.264154],\n",
    "[1.261771,-1.231523,3.531629],\n",
    "[-0.876358,0.487878,-3.354190],\n",
    "[1.067099,-1.458840,2.920181],\n",
    "[0.571671,-0.605419,2.099774],\n",
    "[0.705751,0.193785,3.436813],\n",
    "[-0.362954,0.911640,-0.863548],\n",
    "[0.621957,-0.042161,2.870975],\n",
    "[-1.070071,1.535059,-2.851115],\n",
    "[1.538990,0.229833,5.227305],\n",
    "[-0.408974,-0.043215,-2.031554],\n",
    "[-0.065650,0.048440,-0.279572],\n",
    "[-1.537600,0.363961,-4.633284],\n",
    "[1.335067,-0.679865,4.181856],\n",
    "[-0.163249,0.843809,0.031184],\n",
    "[0.407631,-1.188332,0.793848],\n",
    "[-1.057895,0.098582,-4.258041],\n",
    "[-0.849082,-0.363482,-4.116854],\n",
    "[-0.984360,-1.352803,-5.517394],\n",
    "[-0.251549,1.158084,-0.086438],\n",
    "[-0.387402,-1.116570,-3.005492],\n",
    "[-0.218952,0.820416,-0.265617],\n",
    "[1.365842,-1.167767,3.727585],\n",
    "[-0.792565,-1.275050,-4.835830],\n",
    "[-1.203528,-0.824395,-5.490953],\n",
    "[0.525556,-0.041729,2.466742],\n",
    "[0.703162,-1.110914,2.122252],\n",
    "[-1.564109,0.736335,-4.263553],\n",
    "[0.823868,1.341778,5.010672],\n",
    "[-1.514303,-0.910202,-5.902225],\n",
    "[0.614767,0.464101,3.347941],\n",
    "[-0.527761,1.127692,-1.390311],\n",
    "[-1.008137,0.193929,-4.035269],\n",
    "[-0.806610,-0.421677,-4.031406],\n",
    "[0.411946,0.512661,2.514626],\n",
    "[-0.834414,-1.546277,-5.250791],\n",
    "[0.966863,0.517455,4.632997],\n",
    "[0.844912,0.761885,4.501448],\n",
    "[0.920460,-1.518473,2.460929],\n",
    "[1.057656,1.287705,5.643740],\n",
    "[-0.772096,0.012392,-3.475799],\n",
    "[-0.324844,1.501839,-0.093968],\n",
    "[0.472298,-0.318133,1.956538],\n",
    "[0.667401,-0.274654,2.820081],\n",
    "[-0.547176,0.723871,-1.877516],\n",
    "[0.385916,-1.503229,0.378810],\n",
    "[-0.382848,1.428975,-0.438844],\n",
    "[1.195235,0.179787,4.831296],\n",
    "[-1.440576,0.076244,-4.881423],\n",
    "[-1.043994,1.506872,-2.815223],\n",
    "[-0.511031,1.138046,-1.307339],\n",
    "[0.159654,0.145512,0.940395],\n",
    "[0.462759,-0.541567,1.690525],\n",
    "[0.923337,0.249392,4.237485],\n",
    "[0.524166,-0.658485,1.843971],\n",
    "[1.386838,-0.014357,4.901280],\n",
    "[-1.291396,0.396366,-4.409739],\n",
    "[-1.495416,1.467516,-3.518285],\n",
    "[0.009611,1.087185,1.135241],\n",
    "[-1.095766,-0.686337,-5.132730],\n",
    "[0.079791,-0.525364,-0.126833],\n",
    "[-0.124900,1.360425,0.737550],\n",
    "[-1.529307,0.317558,-4.678139],\n",
    "[0.286399,-1.399110,0.013388],\n",
    "[-1.177019,0.813178,-3.804154],\n",
    "[-1.463010,-0.117278,-5.088261],\n",
    "[0.380403,0.267416,2.123891],\n",
    "[-1.542538,-0.727610,-5.725614],\n",
    "[1.296621,0.951571,5.764816],\n",
    "[1.092554,0.249967,4.688993],\n",
    "[-1.277734,0.908572,-3.878247],\n",
    "[-1.335162,-0.998310,-5.860143],\n",
    "[-0.265882,-1.259806,-2.573608],\n",
    "[-1.101614,1.314502,-3.145190],\n",
    "[1.398152,-0.626607,4.299062],\n",
    "[-0.942655,-0.092878,-4.138484],\n",
    "[0.418801,0.503266,2.536591],\n",
    "[-0.303273,0.460266,-1.032960],\n",
    "[-0.749997,1.553947,-1.854235],\n",
    "[0.631976,-0.356243,2.597459],\n",
    "[0.796304,0.739738,4.313618],\n",
    "[1.097252,-0.394065,4.055722],\n",
    "[-0.563666,1.125486,-1.545957],\n",
    "[0.449097,-0.837865,1.332895],\n",
    "[0.506286,-0.029170,2.395491],\n",
    "[0.982922,0.799324,4.959931],\n",
    "[-0.691418,1.218532,-1.969618],\n",
    "[1.371786,0.613472,5.514786],\n",
    "[1.529499,-1.300600,3.695137],\n",
    "[0.357154,1.356830,3.104876],\n",
    "[-0.356435,0.985271,-0.759406],\n",
    "[-0.306868,-1.508406,-3.018779],\n",
    "[-0.082284,0.960679,0.549725],\n",
    "[0.522632,0.738492,3.234305],\n",
    "[0.943806,-0.416212,3.632772],\n",
    "[-1.188571,0.169289,-4.469896],\n",
    "[-1.247294,1.479501,-3.261139],\n",
    "[1.345996,-0.240188,4.634006]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the dataset to train a NN that takes two inputs (angle, angular velocity) and returns the value of F that solves equation 1 and 2, thus effectively solving the pole balancing differential equation. \n",
    "\n",
    "Before implementing the neural network, let's first split the data into training and test (where we use 20% of the data for the later). Again wrapping things in tensors, since we will be using the data as input to a PyTorch NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "train_data = pole_data[:160]\n",
    "test_data = pole_data[160:]\n",
    "\n",
    "\n",
    "x_train = [i[:2] for i in train_data]\n",
    "y_train = [i[-1] for i in train_data]\n",
    "x_train = torch.tensor(x_train)\n",
    "y_train = torch.tensor(y_train)\n",
    "\n",
    "x_test = [i[:2] for i in test_data]\n",
    "y_test = [i[-1] for i in test_data]\n",
    "x_test = torch.tensor(x_test)\n",
    "y_test = torch.tensor(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's implement the template for a NN called PoleNN. The NN contains one hidden layer consisting of three neurons, each which takes the same two inputs. It applies a Sigmoid activation function on the hidden layer, but only a linear activation function on the output layer (why?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class PoleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PoleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(2,3)\n",
    "        self.fc2 = nn.Linear(3,1)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        hidden = self.fc1(x)\n",
    "        hidden = self.sigmoid(hidden)\n",
    "        output = self.fc2(hidden)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get one instance of the above NN, and call it pole_model. Since we didn't specify any intial weights/biases these will all be randomised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "pole_model = PoleNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we want to train pole_model on our dataset, we need to specify which error loss function we want (in our case it will be sequare error loss - why?) and that we want to optimise using stochastic gradient descent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(pole_model.parameters(), lr = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that when we set up stochastic gradient descent as our optimizer, we specified which parameters we want to optimise, in our case we chose *pole_model.parameters()*, which are all the pole_model parameters, so all weights and biases of all the neuron in the pole_model. We also specified that we want the learning rate to be 0.1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now before we train our model, let's first check it's performace to be able to compare how much it improved after we trained it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss before training 13.2830810546875\n"
     ]
    }
   ],
   "source": [
    "pole_model.eval()\n",
    "y_pred = pole_model(x_test)\n",
    "before_train = criterion(y_pred.squeeze(), y_test)\n",
    "print('Test loss before training' , before_train.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evalute the model, we need to get into what is known as evaluation mode, which is done in the first line.\n",
    "\n",
    "We then obtained the predicted value from the model in the second line, and in the third line we compared the predicted with the actuals using our criteron (which we previously defined as square error loss)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train our model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code, we first enter train mode. We then specify that we want to have 20 training epochs.\n",
    "\n",
    "For each optimise we need to reset the gradient computed in the previous epoch by calling the zero_grad() function. \n",
    "\n",
    "The bulk of the training work is done by PyTorch for us in the lines loss.backward() and optimizer.step(), which computes the error loss and then performs backpropogation using our optimization algorithm of choise (in our case we specified it as SGD previously). Finally it also updates the weights of the model accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train loss: 13.2830810546875\n",
      "Epoch 1: train loss: 12.929855346679688\n",
      "Epoch 2: train loss: 12.75624942779541\n",
      "Epoch 3: train loss: 12.640436172485352\n",
      "Epoch 4: train loss: 12.53436279296875\n",
      "Epoch 5: train loss: 12.416845321655273\n",
      "Epoch 6: train loss: 12.276455879211426\n",
      "Epoch 7: train loss: 12.105316162109375\n",
      "Epoch 8: train loss: 11.897013664245605\n",
      "Epoch 9: train loss: 11.646146774291992\n",
      "Epoch 10: train loss: 11.348516464233398\n",
      "Epoch 11: train loss: 11.001516342163086\n",
      "Epoch 12: train loss: 10.604437828063965\n",
      "Epoch 13: train loss: 10.15863037109375\n",
      "Epoch 14: train loss: 9.667430877685547\n",
      "Epoch 15: train loss: 9.13597583770752\n",
      "Epoch 16: train loss: 8.57097053527832\n",
      "Epoch 17: train loss: 7.980461120605469\n",
      "Epoch 18: train loss: 7.373589515686035\n",
      "Epoch 19: train loss: 6.760279178619385\n"
     ]
    }
   ],
   "source": [
    "pole_model.train()\n",
    "epoch = 20\n",
    "for epoch in range(epoch):\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = pole_model(x_test)\n",
    "    loss = criterion(y_pred.squeeze(),y_test)\n",
    "    print('Epoch {}: train loss: {}'.format(epoch, loss.item()))\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice in the above that our error improves in each epochs.\n",
    "\n",
    "There is a marked improvement in the performance of the model on the training data by the end of the 20th epoch. But how well does it generalise? Let's check this by evaluating the trained model on the test dataset we created previously and comparing it's performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss before training 5.775805473327637\n"
     ]
    }
   ],
   "source": [
    "pole_model.eval()\n",
    "y_pred = pole_model(x_test)\n",
    "before_train = criterion(y_pred.squeeze(), y_test)\n",
    "print('Test loss before training' , before_train.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great there is also a  marked improvement in the performance of the model on the test dataset! \n",
    "\n",
    "Implying that even a simple NN with only one hidden layer is capable of solving sophisticated differential equation, with a relatively small training dataset and a small training run. This highlights the power of multi-layer NN and in particular backpropodation as a means of training them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further exercises:\n",
    "\n",
    "* In our PoleNN template we only had 3 neurons in our hidden layer. Experiment on the impact on the performance if you increase or decrease the number of neurons in the hidden layer.\n",
    "\n",
    "\n",
    "* What about when you increase the number of layers?\n",
    "\n",
    "\n",
    "* Also what is the impact on the performance if you decrease/increase the learning rate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
